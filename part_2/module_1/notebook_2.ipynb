{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тетрадь 2: Преобразователи данных\n",
    "\n",
    "Предметом изучения в данной тетради являются преобразователи данных. Мы разберём, что они \n",
    "из себя представляют, как они устроены и для чего нужны.\n",
    "\n",
    "## Содержание\n",
    "\n",
    "- [Теория](#Теория)\n",
    "- [Примеры](#Примеры)\n",
    "- [Задания](#Задания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория\n",
    "\n",
    "### Преобразование и предварительная обработка данных\n",
    "\n",
    "**Предварительная обработка данных** (Data Preprocessing) в машинном обучении — это набор методов\n",
    "и техник, которые применяются к исходным данным перед их использованием для обучения модели. Цель \n",
    "предварительной обработки — подготовить данные к анализу, улучшить их качество и сделать их более \n",
    "пригодными для обучения моделей машинного обучения.\n",
    "\n",
    "Вообще, существует множество различных этапов предварительной обработки данных. Однако в контексте \n",
    "данного курса мы будем относить всякие манипуляции, будь то очистка, сокращение размерности или \n",
    "форматирование, к преобразованиям данных. Отсюда **преобразователи данных** — программные интерфейсы \n",
    "(в основном классы), которые переводят данные из одного состояние в другое.\n",
    "\n",
    "Проиллюстрируем такой этап предобработки, как дополнение данных. Суть дополнения в том, что \n",
    "информация, которую вы где-либо раздобыли для анализа далеко не всегда является полной (например, в\n",
    "таблице присутствуют пустые поля). Ниже представлена диаграмма, которая схематично описывает эффект\n",
    "применения так называемого `Imputer`'а к данным с пропусками.\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../images/imputer_scheme_plot.png\" alt=\"Imputer Scheme\">\n",
    "</div>\n",
    "\n",
    "### Некоторые этапы предварительной обработки данных\n",
    "\n",
    "Возвращась к разнообразию этапов предварительной обработки данных, пречислим некоторые из них: \n",
    "\n",
    "- **Разделение:** тренировочный/тестовый/валидационный набор; кросс-валидация.\n",
    "\n",
    "- **Очистка:** удаление дубликатов; обработка пропущенных значений; удаление шума.\n",
    "\n",
    "- **Масштабирование:** нормализация/Стандартизация (например, Min-Max нормализация, \n",
    "  Z-масштабирование); логарифмирование/экспоненцирование.\n",
    "\n",
    "- **Кодирование:** бинарное кодирование (One-Hot Encoding); лэйбл-кодирование (Label Encoding).\n",
    "\n",
    "- **Уменьшение Размерности:** PCA (Principal Component Analysis); t-SNE (t-distributed Stochastic \n",
    "  Neighbor Embedding); LDA (Linear Discriminant Analysis).\n",
    "\n",
    "### Порядок выполнения этапов предварительной обработки\n",
    "\n",
    "Очень важно помнить, что порядок выполнения этих этапов действительно имеет огромное значение.\n",
    "Сначала выполняется разделение данных, затем очистка, а потом уже всё остальное. Также далее мы \n",
    "обсудим не менее значимый нюанс касаемо применения преобразователей к отдельным наборам данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры\n",
    "\n",
    "Рассмотрим теперь несколько упрощённых и один полный пример преобразователей данных. Начнём с \n",
    "более простых функций, а далее продемонстрируем полноценный класс. В качетве преобразования для \n",
    "примера возьмём Z-масштабирование. Формула для этой операции выглядит следующим образом:\n",
    "\n",
    "$$Z(X) = \\frac{X - \\textrm{mean}(X)}{\\textrm{std}(X)},$$\n",
    "\n",
    "где:\n",
    "- $X$ — множество всех точек данных признака;\n",
    "- $\\textrm{mean}(X)$ — среднее по $X$;\n",
    "- $\\textrm{std}(X)$ — стандартное отклонение по $X$.\n",
    "\n",
    "Запрограммируем функцию для Z-масштабирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "def z_scale(data: ndarray) -> ndarray:\n",
    "    scaled = (data - np.mean(data)) / np.std(data)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь представим некоторый случайный набор данных и применим к нему нашу функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "[[ 8.540e-01  1.625e+00 -2.760e-01  9.670e-01]\n",
      " [-2.930e-01  1.800e-01  1.384e+00  2.030e-01]\n",
      " [-1.322e+00 -2.000e-03 -2.270e-01 -1.630e+00]\n",
      " [ 3.900e-02  6.050e-01 -2.510e-01  6.990e-01]\n",
      " [-5.040e-01 -5.930e-01  1.086e+00  1.199e+00]\n",
      " [-7.040e-01 -2.401e+00 -1.371e+00  1.034e+00]\n",
      " [ 6.750e-01 -4.660e-01 -4.920e-01  2.140e-01]\n",
      " [-4.720e-01  1.098e+00  3.420e-01 -1.643e+00]\n",
      " [ 1.152e+00  1.555e+00  3.800e-02 -3.950e-01]\n",
      " [ 2.260e-01  3.670e-01  6.050e-01  1.504e+00]]\n",
      "\n",
      "After scaling:\n",
      "[[ 0.783  1.599 -0.414  0.902]\n",
      " [-0.432  0.069  1.344  0.093]\n",
      " [-1.522 -0.124 -0.362 -1.849]\n",
      " [-0.081  0.519 -0.388  0.618]\n",
      " [-0.656 -0.75   1.028  1.148]\n",
      " [-0.868 -2.665 -1.574  0.973]\n",
      " [ 0.593 -0.616 -0.643  0.105]\n",
      " [-0.622  1.041  0.24  -1.862]\n",
      " [ 1.098  1.525 -0.082 -0.54 ]\n",
      " [ 0.117  0.267  0.519  1.471]]\n"
     ]
    }
   ],
   "source": [
    "(size, round_to) = ((10, 4), 3)\n",
    "selection = np.round(np.random.standard_normal(size), round_to)\n",
    "scaled = np.round(z_scale(selection), round_to)\n",
    "\n",
    "print(f\"Before scaling:\\n{selection}\\n\\nAfter scaling:\\n{scaled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, всё работает отлично! Однако, есть нюанс, о котором было упомянуто в конце теоретического \n",
    "раздела, и сейчас мы его обусдим.\n",
    "\n",
    "Итак, представьте, что вы обучаете модель на тренировочных данных. Вы готовите её к взаимодействию \n",
    "с информацией, которую она ещё не видела. Но во время преобразования данных вы применяете инструмент \n",
    "трансформации к каждому из наборов (тренировочному и тестовому) по отдельности. То есть та же функция \n",
    "`z_scale()` принимает во внимание характеристики тестового набора данных при его масштабировании. \n",
    "Теперь представим, что тестового набора нет. Вам только предстоит его получить и обработать. \n",
    "Получается, для преобразования тестового набора данных нам нужно заглянуть в будущее!\n",
    "\n",
    "Многие начинающие часто совершают эту ошибку. Новички трансформируют тестовые данные, пользуясь их \n",
    "характеристиками. Хоть это и явное противоречие, часто его никто не замечает. Так что же делать с \n",
    "этим противоречием? \n",
    "\n",
    "Для решения этой проблемы нам понадобится хранить параметры тренировочных данных (минимум, максимум, \n",
    "среднее, медиану, стандартное отклонение и т.п.) и использовать их для преобразований тестовых. \n",
    "Рассмотрим конкретную реализацию.\n",
    "\n",
    "Для начала зададим интерфейс преобразователя в той форме, в которой нам будет удобно им пользоваться.\n",
    "За основу возьмём те же классы-трансформеры из библиотеки Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Tuple\n",
    "\n",
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    # The Base Preprocessor class is an abstract base class for preprocessor\n",
    "    # implementations.\n",
    "\n",
    "    def __init__(self, copy: bool = True) -> None:\n",
    "        self.copy = copy\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, x: ndarray) -> None:\n",
    "        # Fit the preprocessor to the provided features.\n",
    "        message = \"Every preprocessor must implement the `fit()` method.\"\n",
    "        raise NotImplementedError(message)\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, x: ndarray) -> Any:\n",
    "        # Transform the input features.\n",
    "        message = \"Every preprocessor must implement the `transform()` method.\"\n",
    "        raise NotImplementedError(message)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_values_masks(array: ndarray) -> Tuple[bool, bool]:\n",
    "        non_zero_values_mask = array != 0\n",
    "        zero_values_mask = ~non_zero_values_mask\n",
    "        return non_zero_values_mask, zero_values_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, это [абстрактный базовый класс](https://docs.python.org/3/library/abc.html). При \n",
    "инициализации мы задаём параметр `copy`, который нам пригодится в дальнейшем. Главное — у класса \n",
    "`BasePreprocessor` есть методы `fit()` и `transform()`. Первый отвечает за хранение характеристик \n",
    "данных, а второй за применение транформации с учётом самых характеристик. Есть также защищённый \n",
    "метод `_get_values_masks()`. О нём мы тоже поговорим чуть позже.\n",
    "\n",
    "Рассмотрим теперь конкретную реализаицю Z-масштабирующего преобразователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScalingPreprocessor(BasePreprocessor):\n",
    "    # Z-Scaling Preprocessor inherits from Base Preprocessor to provide method\n",
    "    # signatures and provide calls to `_get_values_masks()` through self-ref.\n",
    "\n",
    "    def __init__(self, copy: bool = True) -> None:\n",
    "        super().__init__(copy)\n",
    "        self.means: ndarray\n",
    "        self.stds: ndarray\n",
    "\n",
    "    def fit(self, x: ndarray) -> None:\n",
    "        # This method memotizes the data parameters of the input as attributes\n",
    "        # to use this information for Z-Scale calculation.\n",
    "        self.means = np.nanmean(x, axis=0)\n",
    "        self.stds = np.nanstd(x, axis=0)\n",
    "\n",
    "    def transform(self, x: ndarray) -> ndarray:\n",
    "        # This method applies the scaling formula excluding zero elements from\n",
    "        # the process.\n",
    "\n",
    "        # Use the copy if `copy` parameter was specifyed\n",
    "        if self.copy:\n",
    "            x = x.copy()\n",
    "\n",
    "        # Get zero and nonzero elements positions to avoid artefacts\n",
    "        (nonzero_std_mask, zero_std_mask) = self._get_values_masks(self.stds)\n",
    "        (nonzero_mean_mask, _) = self._get_values_masks(self.means)\n",
    "        x[:, zero_std_mask] = 0\n",
    "\n",
    "        # Use the Z-Scale formula\n",
    "        x[:, nonzero_std_mask] = (\n",
    "            x[:, nonzero_std_mask] - self.means[nonzero_mean_mask]\n",
    "        ) / self.stds[nonzero_std_mask]\n",
    "        return x\n",
    "\n",
    "    def fit_transform(self, x) -> ndarray:\n",
    "        # Fit and transform at the same time.\"\"\"\n",
    "\n",
    "        self.fit(x)\n",
    "\n",
    "        transformed = self.transform(x)\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проясним некоорые моменты более подробно:\n",
    "\n",
    "- При вычислении среднего и стандартного отклочения в методе `fit()`, мы используем функции с \n",
    "  приставкой `nan*`. Это значит, что при подсчётах ячейки, содержащие `NaN` или любые другие \n",
    "  недействительные или нечисловые значения, будут проигнорированы.\n",
    "- Перед масштабированием с применением формулы мы отсекаем точки данных, что представляют собой \n",
    "  нули, для того чтобы избежать артефактов в виде отрицательных значений.\n",
    "\n",
    "Теперь посмотрим, насколько удачно можно применить вышеописанный интерфейс.\n",
    "\n",
    "Обязательно выполните следующую ячейку! В противном случае возникнет `ModuleNotFoundError` при \n",
    "попытке проверить правильность выполнения задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём непосредственно к демонстрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-train:\n",
      "[[-1.322 -0.002 -0.227]\n",
      " [-0.504 -0.593  1.086]\n",
      " [-0.472  1.098  0.342]\n",
      " [ 1.152  1.555  0.038]\n",
      " [ 0.039  0.605 -0.251]\n",
      " [-0.293  0.18   1.384]\n",
      " [ 0.854  1.625 -0.276]\n",
      " [ 0.675 -0.466 -0.492]]\n",
      "\n",
      "X-test:\n",
      "[[ 0.226  0.367  0.605]\n",
      " [-0.704 -2.401 -1.371]]\n",
      "\n",
      "y-train:\n",
      "[[-1.63 ]\n",
      " [ 1.199]\n",
      " [-1.643]\n",
      " [-0.395]\n",
      " [ 0.699]\n",
      " [ 0.203]\n",
      " [ 0.967]\n",
      " [ 0.214]]\n",
      "\n",
      "y-test:\n",
      "[[1.504]\n",
      " [1.034]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from module_1 import DataSplitter\n",
    "\n",
    "\n",
    "splitter = DataSplitter(permute=True)\n",
    "scaler = ZScalingPreprocessor(copy=True)\n",
    "\n",
    "(x, y) = (selection[:, :-1], selection[:, -1].reshape((-1, 1)))\n",
    "(x_train, x_test, y_train, y_test) = splitter.split_data(x, y, test_size=0.25)\n",
    "\n",
    "nonscaled = (x_train, x_test, y_train, y_test)\n",
    "names = (\"X-train\", \"X-test\", \"y-train\", \"y-test\")\n",
    "for nscld, name in zip(nonscaled, names):\n",
    "    print(f\"{name}:\\n{nscld}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-train scaled:\n",
      "[[-1.724 -0.621 -0.663]\n",
      " [-0.67  -1.352  1.374]\n",
      " [-0.629  0.739  0.22 ]\n",
      " [ 1.463  1.305 -0.252]\n",
      " [ 0.029  0.13  -0.7  ]\n",
      " [-0.398 -0.396  1.836]\n",
      " [ 1.079  1.391 -0.739]\n",
      " [ 0.849 -1.195 -1.074]]\n",
      "\n",
      "X-test scaled:\n",
      "[[ 0.27  -0.165  0.627]\n",
      " [-0.928 -3.588 -2.438]]\n",
      "\n",
      "y-train scaled:\n",
      "[[-1.54 ]\n",
      " [ 1.214]\n",
      " [-1.552]\n",
      " [-0.338]\n",
      " [ 0.727]\n",
      " [ 0.245]\n",
      " [ 0.988]\n",
      " [ 0.255]]\n",
      "\n",
      "y-test scaled:\n",
      "[[1.511]\n",
      " [1.054]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled = (\n",
    "    scaler.fit_transform(x_train),\n",
    "    scaler.transform(x_test),\n",
    "    scaler.fit_transform(y_train),\n",
    "    scaler.transform(y_test),\n",
    ")\n",
    "\n",
    "names = (\"X-train scaled\", \"X-test scaled\", \"y-train scaled\", \"y-test scaled\")\n",
    "for scld, name in zip(scaled, names):\n",
    "    print(f\"{name}:\\n{np.round(scld, 3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "### Задание 1\n",
    "\n",
    "По аналогии с заданием в примерах реализуйте класс для MinMax масштабирования, наследуя от базового \n",
    "класса `BasePreprocessor`. В качестве подсказки рассмотрите следующую формулу:\n",
    "\n",
    "$$ \\textrm{MinMax(X)} = \\frac{X - \\textrm{min}(X)}{\\textrm{max}(X) - \\textrm{min}(X)}, $$\n",
    "\n",
    "где:\n",
    "- $X$ — множество всех точек данных признака;\n",
    "- $\\textrm{min}(X)$ — минимум по $X$;\n",
    "- $\\textrm{max}(X)$ — максимум по $X$.\n",
    "\n",
    "В процессе решения не забудьте про маски для нулевых значений!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMScalingPreprocessor(BasePreprocessor):\n",
    "    # MinMax Scaling Preprocessor inherits from Base Preprocessor to provide\n",
    "    # method signatures and provide calls to `_get_values_masks()` through\n",
    "    # self-ref.\n",
    "\n",
    "    def __init__(self, copy: bool = True) -> None:\n",
    "        super.__init__(copy)\n",
    "        self.min_values: ndarray\n",
    "        self.max_values: ndarray\n",
    "\n",
    "    def fit(self, x: ndarray) -> None:\n",
    "        # This method should fit the transformer to the data.\n",
    "        ...\n",
    "\n",
    "    def transform(self, x: ndarray) -> Any:\n",
    "        # This method should transform the data accordingly to the state,\n",
    "        # produced by the :meth:`fit()` method.\n",
    "        ...\n",
    "\n",
    "    def fit_transform(self, x: ndarray) -> Any:\n",
    "        # This method just combines both :meth:`fit()` and :meth:`transform()`.\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда реализуете класс из ячейки выше, запускайте ячейку ниже и продемонстрируйте правильность \n",
    "выполнения задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor '__init__' requires a 'super' object but received a 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m splitter \u001b[38;5;241m=\u001b[39m DataSplitter(permute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mMMScalingPreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m (selection[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], selection[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m      5\u001b[0m (x_train, x_test, y_train, y_test) \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_data(\n\u001b[0;32m      6\u001b[0m     x, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2024\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mMMScalingPreprocessor.__init__\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_values: ndarray\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_values: ndarray\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor '__init__' requires a 'super' object but received a 'bool'"
     ]
    }
   ],
   "source": [
    "splitter = DataSplitter(permute=True)\n",
    "scaler = MMScalingPreprocessor(copy=True)\n",
    "\n",
    "(x, y) = (selection[:, :-1], selection[:, -1].reshape((-1, 1)))\n",
    "(x_train, x_test, y_train, y_test) = splitter.split_data(\n",
    "    x, y, test_size=0.25, random_seed=2024\n",
    ")\n",
    "\n",
    "nonscaled = (x_train, x_test, y_train, y_test)\n",
    "names = (\"X-train\", \"X-test\", \"y-train\", \"y-test\")\n",
    "for nscld, name in zip(nonscaled, names):\n",
    "    print(f\"{name}:\\n{nscld}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2*\n",
    "\n",
    "Данное задание обладает повышенной сложностью выполнения. Тут вам предстоит самостоятельно и без\n",
    "каких-либо подсказок реализовать класс `ImputingPreprocessor`, который будет иметь функционал,\n",
    "описанный в [теории](#теория). Данный интерфейс должен быть способен выполнить замену недостающих\n",
    "значений (например, `NaN`) по трём стратегиям: константа, среднее и медиана по столбцу. **Важно\n",
    "помнить, что все дополнения воспроизводятся относительно столбцов!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputingPreprocessor(BasePreprocessor):\n",
    "    # This class should implement an interface for imputing the missing values\n",
    "    # by one of these strategies: \"constant\", \"mean\" and \"median\".\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и в случае с предыдущим заданием, выполните ячейку ниже, когда закончите выполнять\n",
    "задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class ImputingPreprocessor without an implementation for abstract methods 'fit', 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(na_shape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m      7\u001b[0m         nan_array[i, j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;28;01mif\u001b[39;00m nprand() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nan_array[i, j]\n\u001b[1;32m----> 9\u001b[0m imputer \u001b[38;5;241m=\u001b[39m \u001b[43mImputingPreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m na_imputed \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(nan_array)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore imputing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnan_array\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class ImputingPreprocessor without an implementation for abstract methods 'fit', 'transform'"
     ]
    }
   ],
   "source": [
    "from numpy.random import random as nprand\n",
    "\n",
    "\n",
    "na_shape = (5, 5)\n",
    "nan_array = np.random.standard_normal(na_shape)\n",
    "for i in range(na_shape[0]):\n",
    "    for j in range(na_shape[1]):\n",
    "        nan_array[i, j] = np.nan if nprand() > 0.8 else nan_array[i, j]\n",
    "\n",
    "imputer = ImputingPreprocessor(...)\n",
    "na_imputed = imputer.fit_transform(nan_array)\n",
    "\n",
    "print(f\"Before imputing:\\n{nan_array}\\n\")\n",
    "print(f\"After imputing:\\n{na_imputed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "В настоящей тетради мы: поближе познакомились с предварительной обработкой данных и некоторыми\n",
    "её этапами; изучили, что такое преобразователи данных, для чего они нужны, как ими пользоваться,\n",
    "а самое главное — научились самостоятельно их воспроизводить!\n",
    "\n",
    "Если есть интерес, можете попробовать далее модифицровать код из секций выше. Также можете \n",
    "попробовать написать собственный интерфейс, который вам покажется более удобным."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratches",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
