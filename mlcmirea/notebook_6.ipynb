{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тетрадь 6: Алгоритм k-ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория\n",
    "\n",
    "Алгоритм k-ближайших соседей KNN — это один из самых простых и интуитивно понятных алгоритмов классификации. Он относится к\n",
    "методам **ленивого обучения** (lazy learning), так как не строит модель на этапе обучения, а\n",
    "откладывает все вычисления до момента классификации нового объекта.\n",
    "\n",
    "#### Основные шаги алгоритма\n",
    "\n",
    "1. **Выбор числа соседей (k)**\n",
    "   - Параметр k определяет, сколько ближайших соседей будет учитываться при классификации.\n",
    "   - Обычно выбирается нечётное число, чтобы избежать ничьей при голосовании.\n",
    "\n",
    "2. **Вычисление расстояний**\n",
    "   - Для нового объекта вычисляются расстояния до всех объектов в обучающей выборке.\n",
    "   - Часто используются метрики расстояния, такие как евклидово расстояние, манхэттенское расстояние\n",
    "   или косинусное сходство.\n",
    "\n",
    "3. **Выбор k ближайших соседей**\n",
    "   - Из обучающей выборки выбираются $k$ объектов, которые находятся ближе всего к новому объекту.\n",
    "\n",
    "4. **Голосование**\n",
    "   - Класс нового объекта определяется путём голосования среди $k$ ближайших соседей.\n",
    "   - Например, если $k=3$, и два соседа принадлежат к классу $A$, а один — к классу $B$, то новый\n",
    "   объект будет отнесён к классу $A$.\n",
    "\n",
    "5. **Возвращение результата**:\n",
    "   - Алгоритм возвращает предсказанный класс для нового объекта.\n",
    "\n",
    "#### Преимущества:\n",
    "\n",
    "- Простота реализации и интерпретации.\n",
    "- Не требует этапа обучения (все вычисления происходят на этапе предсказания).\n",
    "- Хорошо работает на данных с небольшим количеством признаков.\n",
    "\n",
    "#### Недостатки:\n",
    "\n",
    "- Вычислительная сложность на больших данных, так как для каждого нового объекта нужно вычислять\n",
    "расстояния до всех объектов в обучающей выборке.\n",
    "- Чувствительность к масштабированию данных (необходимо нормализовать признаки).\n",
    "- Чувствительность к выбору параметра $k$: слишком маленькое $k$ может привести к переобучению, а\n",
    "слишком большое — к недообучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "class LoopingKNNClassifier(ModelBase):\n",
    "    # This K-Nearest Neighbors Classifier loops twice, comparing each train and\n",
    "    # test point in the set. Uses Euclidean distance to determine the nearest\n",
    "    # neighbors.\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, x_train: ndarray, y_train: ndarray, *args, **kwargs) -> None:\n",
    "        super().fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x_test, n_neighbors: int = 5):\n",
    "        # This method is where the distances calculation happens.\n",
    "        n_test_samples = x_test.shape[0]\n",
    "        n_train_samples = x_train.shape[0]\n",
    "        distances = np.zeros((n_test_samples, n_train_samples))\n",
    "\n",
    "        # Iterating over each test sample,\n",
    "        for i in range(n_test_samples):\n",
    "            # iterate over each training sample,\n",
    "            for j in range(n_train_samples):\n",
    "                # calculate the Euclidean distance between the current test\n",
    "                # sample (`x_test[i]`) and the current training sample\n",
    "                # (`x_train[j]`).\n",
    "                distances[i, j] = np.sqrt(\n",
    "                    np.sum(np.square(x_test[i, :] - x_train[j, :]))\n",
    "                )\n",
    "\n",
    "        # Predict the lables based on the distances calculated.\n",
    "        predicted_labels = self._predict_labels(distances, n_neighbors)\n",
    "        return predicted_labels\n",
    "\n",
    "    def _predict_labels(\n",
    "        self,\n",
    "        distances: ndarray,\n",
    "        n_neighbors: int = 5,\n",
    "    ) -> ndarray:\n",
    "        # Predicting labels is a separate process. That's why it has it's own\n",
    "        # private method.\n",
    "        n_test_samples = distances.shape[0]\n",
    "        y_predicted = np.zeros((n_test_samples, 1))\n",
    "\n",
    "        # Iterating over each test sample to predict its label,\n",
    "        for i in range(n_test_samples):\n",
    "            # sort the distances for the current test sample and get the\n",
    "            # indices of the sorted distances,\n",
    "            sorted_indices = np.argsort(distances[i, :])\n",
    "            # determine the number of neighbors to consider, ensuring it does\n",
    "            # not exceed the available neighbors,\n",
    "            farthest_neighbor = np.min([n_neighbors, len(sorted_indices)])\n",
    "            # get the labels of the closest neighbors from the training set,\n",
    "            closest_neighbors = self.y_train[\n",
    "                sorted_indices[:farthest_neighbor]\n",
    "            ]\n",
    "            # use the mode (most frequent label) of the closest neighbors as\n",
    "            # the predicted label.\n",
    "            y_predicted[i] = mode(closest_neighbors)[0]\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
