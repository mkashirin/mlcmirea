{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тетрадь 3: Оценка точности моделей классификации\n",
    "\n",
    "# Содержание\n",
    "\n",
    "- [Введение](#Ввдение)\n",
    "- [Код и примеры](#Код-и-примеры)\n",
    "- [Задания](#Задания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "### Метрики\n",
    "\n",
    "В контексте машинного обучения (ML) и искусственного интеллекта (AI) **метрики** - это\n",
    "количественные показатели, используемые для оценки производительности, результативности и качества\n",
    "моделей. Эти показатели помогут нам понять, насколько хорошо модель справляется с поставленной\n",
    "задачей. В зависимости от типа решаемой задачи используются различные показатели (например,\n",
    "классификация, регрессия, кластеризация).\n",
    "\n",
    "### Показатели классификации\n",
    "\n",
    "Классификация - это распространенная задача в ML, целью которой является прогнозирование категориальной метки для заданных входных данных. Общие показатели для оценки моделей классификации включают:\n",
    "\n",
    "#### Точность (Accuracy)\n",
    "\n",
    "Точность суть доля правильно предсказанных экземпляров из общего числа экземпляров. Она\n",
    "определяется как:\n",
    "\n",
    "$$ \\textrm{Acc}(Y_n, Y_N) = \\frac{Y_n}{Y_N}, $$\n",
    "где:\n",
    "* $Y_n$ — количество правильных предсказаний;\n",
    "* $Y_N$ — общее количство предсказаний.\n",
    "\n",
    "Точность очевидна, но может вводить в заблуждение, особенно в несбалансированных наборах данных,\n",
    "где доминирует один класс.\n",
    "\n",
    "#### Матрица ошибок (Confusion Matrix)\n",
    "\n",
    "Матрица ошибок — это таблица, которая суммирует эффективность классификационной модели, показывая\n",
    "количество истинно положительных (TP), истинно отрицательных (TN)\n",
    "ложноположительных (FP) и ложноотрицательных (FN) прогнозов. Из этой матрицы\n",
    "могут быть получены другие показатели. В качестве примера рассмотрим две иллюстрации матрицы ошибок:\n",
    "элементарную и сложную (в которой классов больше двух). Вот элементарная матрица ошибок, где всего\n",
    "два класса (1 и 0):\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"images/confusion_matrices_plot.png\" alt=\"Confusion Matrices\">\n",
    "</div>\n",
    "\n",
    "В таблице слева, мы видим, что:\n",
    "* TP (True Positive) — Значение класса суть 0 и предсказание модели есть 0; \n",
    "* TN (True Negative) — Значение класса есть 1 и предсказание модели суть 1;\n",
    "* FP (False Positive) — Значение класса суть 1, а предсказание модели есть 0;\n",
    "* FN (False Negative) — Значение класса есть 0, а предсказание модели суть 1.\n",
    "\n",
    "Таким образом, cправа уже матрица, в которой 3 TP, 5 TN, 1 FP и 2 FN.\n",
    "\n",
    "#### Прецизионность (Precision)\n",
    "\n",
    "Прецизионность есть доля истинно положительных прогнозов из всех положительных прогнозов (как\n",
    "истинных, так и ложноположительных). Определяется как:\n",
    "$$ \\textrm{Prec} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}} $$\n",
    "Прецизионность полезна, когда высока вероятность ложных срабатываний.\n",
    "\n",
    "#### Отзывчивость (Recall)\n",
    "\n",
    "Отзывчивость (чувствительность или Истинно положительный показатель) — это доля истинно\n",
    "положительных прогнозов из всех реальных положительных примеров. Определяется как:\n",
    "$$ \\textrm{Rec} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}} $$\n",
    "Отзывчивость важна, когда стоимость ложноотрицательных результатов высока.\n",
    "\n",
    "#### Специфичность (Specificity)\n",
    "\n",
    "Показатель специфичности особенно полезен, когда высока вероятность ложных срабатываний (ошибочной\n",
    "классификации отрицательного экземпляра как положительного). Формула для спцифичности выглядит так:\n",
    "$$ \\textrm{Spec} = \\frac{\\textrm{TN}}{\\textrm{TN} + \\textrm{FP}} $$\n",
    "Специфичность является важнейшим показателем в сценариях, где класс отрицательных результатов имеет\n",
    "особое значение, и минимизация ложных срабатываний имеет решающее значение.\n",
    "\n",
    "#### Показатель F1 (F1 Score)\n",
    "\n",
    "Показатель F1 суть среднее гармоническое значение прецизионности и отзывчивости. Он определяется\n",
    "как:\n",
    "$$\n",
    "\\textrm{F}_1 = 2 \\times \\frac{\\textrm{Prec} \\times \\textrm{Rec}}{\\textrm{Prec} + \\textrm{Rec}}\n",
    "$$\n",
    "Оценка F1 будет полезна, если мы захотим сбалансировать прецизионность и отзывчивость.\n",
    "\n",
    "#### Кривая ROC\n",
    "\n",
    "Кривая ROC (кривая рабочих характеристик приемника) есть график, иллюстрирующий диагностические\n",
    "возможности системы бинарного классификатора при изменении ее порога распознавания. Кривая строится\n",
    "путем сопоставления истинно положительной частоты (TPR) и ложноположительной частоты (FPR) при\n",
    "различных пороговых значениях.\n",
    "\n",
    "```shell\n",
    "TODO: ROC graph illustration\n",
    "```\n",
    "\n",
    "#### AUC\n",
    "\n",
    "Площадь под кривой ROC (тут уже ничего пояснять не нужно). Более высокий показатель AUC указывает\n",
    "на налучшую производительность модели, при изменении каждой из пороговых настроек.\n",
    "\n",
    "#### Перекрестная проверка (Cross Validation) \n",
    "\n",
    "Перекрёстный метод оценки того, насколько модель обобщается для независимого набора данных. Он\n",
    "включает в себя разбиение данных на несколько групп и обучение/тестирование модели на разных\n",
    "подмножествах.\n",
    "\n",
    "Рассмотрим теперь на примерах применения некоторых из вышеописанных метрик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код и примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность (Accuracy)\n",
    "\n",
    "Метод оценки точности модели довольно тривиален. Нам лишь нужно поделить количество правильных\n",
    "предсказаний на общее количество предсказаний. Сделать это можно следующим образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(actual: ndarray, predicted: ndarray) -> float:\n",
    "    accuracy = np.sum(predicted == actual) / len(actual)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы нам было, что оценивать, сгенерируем два массива. Один из них будет «действительным», а\n",
    "второй — «предсказанным». А затем применим нашу функцию вычисления точности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual, y_pred = (\n",
    "    np.array([random.randint(0, 1) for _ in range(10)]),\n",
    "    np.array([random.randint(0, 1) for _ in range(10)]),\n",
    ")\n",
    "accuracy = compute_accuracy(y_actual, y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это было довольно просто. Рассмотрим теперь чуть более сложный пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndicesMap = Dict[Any, int]\n",
    "ConfusionMatrix = Tuple[ndarray, IndicesMap]\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(\n",
    "    actual: ndarray,\n",
    "    predicted: ndarray,\n",
    "    indices_map: Optional[IndicesMap] = None,\n",
    ") -> ConfusionMatrix:\n",
    "\n",
    "    # We need to map all the outcomes to integers to place them as rows and\n",
    "    # columns in the confusion matrix\n",
    "    def _map_to_integers(array, intmap):\n",
    "        for i, _ in enumerate(array):\n",
    "            array[i] = intmap[array[i]]\n",
    "        return array\n",
    "\n",
    "    # Then the arrays of actual and predicted values must be converted into\n",
    "    # lists, so that we can treat those values as a common Python data structure\n",
    "    actual_list, predicted_list = (\n",
    "        list(chain.from_iterable(actual.tolist())),\n",
    "        list(chain.from_iterable(predicted.tolist())),\n",
    "    )\n",
    "    concatenated = actual_list + predicted_list\n",
    "    n_features = len(set(concatenated))\n",
    "    if indices_map is None:\n",
    "        # If there is no indicies map provided, we create a default one\n",
    "        indices_map = {\n",
    "            key: val for key, val in zip(set(concatenated), range(n_features))\n",
    "        }\n",
    "\n",
    "    confusion_matrix = np.zeros((n_features, n_features))\n",
    "    mapped_actual, mapped_predicted = (\n",
    "        _map_to_integers(actual_list, indices_map),\n",
    "        _map_to_integers(predicted_list, indices_map),\n",
    "    )\n",
    "    # Finally, the ocurrances of each value get summed up in the corresponding\n",
    "    # matrix cells\n",
    "    for a, p in zip(mapped_actual, mapped_predicted):\n",
    "        confusion_matrix[a, p] += 1\n",
    "    confusion_matrix_with_map: ConfusionMatrix = confusion_matrix, indices_map\n",
    "\n",
    "    return confusion_matrix_with_map\n",
    "\n",
    "\n",
    "y_actual, y_pred = (\n",
    "    np.array([random.randint(0, 2) for _ in range(10)]).reshape((-1, 1)),\n",
    "    np.array([random.randint(0, 2) for _ in range(10)]).reshape((-1, 1)),\n",
    ")\n",
    "confusion_matrix, _ = compute_confusion_matrix(y_actual, y_pred)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы имеем некоторое представление о метриках для моделей классификации и реализующих\n",
    "их функциях, приступим к выполнению задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Рассмотрев примеры выше, можно понять принцип постоения данных функций оценивания. Таким образом,\n",
    "в качестве задания теперь дополните следующую функцию для вычисления чувствительности и\n",
    "специфичности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sensitivities_and_specificities(\n",
    "    actual: ndarray, predicted: ndarray, as_array: bool = True\n",
    ") -> Any:\n",
    "    # Obtain the confusion matrix and the indicies map to compute true\n",
    "    # positives, true negatives, false positives and false negatives\n",
    "    confusion_matrix, indices_map = compute_confusion_matrix(actual, predicted)\n",
    "    n_features = len(indices_map)\n",
    "\n",
    "    sensitivities, specificities = (list(), list())\n",
    "    for i in range(n_features):\n",
    "        # Compute sensetivities\n",
    "        true_positives = ...\n",
    "        false_negatives = ...\n",
    "        sensitivity = true_positives / (true_positives + false_negatives)\n",
    "        sensitivities.append(sensitivity)\n",
    "\n",
    "        # Compute specificities\n",
    "        upper_left = ...\n",
    "        upper_right = ...\n",
    "        lower_left = ...\n",
    "        lower_right = ...\n",
    "        true_negatives = np.sum(\n",
    "            (upper_left, upper_right, lower_left, lower_right)\n",
    "        )\n",
    "        false_positives = np.sum(confusion_matrix[i])\n",
    "        specificity = true_negatives / (true_negatives + false_positives)\n",
    "        specificities.append(specificity)\n",
    "\n",
    "    # In case we do not want to recieve the resulting data as a NumPy array,\n",
    "    # the data must be serialized as a Python data structure\n",
    "    if not as_array:\n",
    "        features_names = list(indices_map.keys())\n",
    "        keys = [\"sensitivities\", \"specificities\"]\n",
    "        metrics = sensitivities, specificities\n",
    "        sensitivities_and_specificities = dict.fromkeys(keys)\n",
    "        for outer_key, metric in zip(keys, metrics):\n",
    "            sensitivities_and_specificities[outer_key] = {\n",
    "                key: val for key, val in zip(features_names, metric)\n",
    "            }\n",
    "        return sensitivities_and_specificities\n",
    "\n",
    "    # Otherwise, we return NumPy arrays as is\n",
    "    sensitivities_and_specificities = np.array([sensitivities, specificities])\n",
    "    return sensitivities_and_specificities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как восполните пропуски в дефиниции функции выше, запустите следующую ячейку, чтобы\n",
    "проверить правильность решения задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual, y_pred = (\n",
    "    np.array([random.randint(0, 2) for _ in range(10)]).reshape((-1, 1)),\n",
    "    np.array([random.randint(0, 2) for _ in range(10)]).reshape((-1, 1)),\n",
    ")\n",
    "sensitivities_and_specificities = compute_sensitivities_and_specificities(\n",
    "    y_actual, y_pred\n",
    ")\n",
    "\n",
    "print(sensitivities_and_specificities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же можете попробовать реализовать другие функции для вычисления метрик, упомянутых в\n",
    "[введении](#Введение)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "Метрики являются важным инструментом в области ОД и искусственного интеллекта для оценки\n",
    "эффективности моделей, руководства при выборе моделей и обеспечения того, чтобы модели хорошо\n",
    "обобщались на новые данные.\n",
    "\n",
    "Выбор метрики зависит от конкретной проблемы, характера данных и желаемых результатов. Понимание\n",
    "этих показателей позволяет практикам принимать обоснованные решения и повышать эффективность своих\n",
    "моделей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratches",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
